{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit61209d9bcfea452998e2771b9ee1a7fb",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Factoid\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from functions_factoid import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path test, pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path_test = \"../data/test_8b.json\"\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"../transformers_models/biobert_factoid_pytorch\")\n",
    "encoder = TFBertModel.from_pretrained(\n",
    "    \"../transformers_models/biobert_factoid_pytorch\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "test_execution = -1\n",
    "learning_rate = 5e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "...Model creation\n"
    }
   ],
   "source": [
    "# Create a new model instance\n",
    "model = model_creation(max_len, learning_rate, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the weights\n",
    "model.load_weights(\n",
    "    'C:/Users/mgabr/Desktop/HLT/project/grid_factoid/ModelloFINALE/runs/20200826_105642/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "...Encode dataset\n"
    }
   ],
   "source": [
    "# Data\n",
    "x_data_test, _, answer_list_test = encode_dataset(\n",
    "    dataset_path_test, max_len, tokenizer, test_execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question and answer dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_question=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_raw=x_data_test[0][n_question]\n",
    "question_raw=tokenizer.convert_ids_to_tokens(question_raw)\n",
    "questio_raw_str=('').join(question_raw)\n",
    "answer=answer_list_test[n_question]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with the first token.\n",
    "answer_start=0\n",
    "answer_end=len(question_raw)-1\n",
    "question = question_raw[0]\n",
    "\n",
    "# Select the remaining answer tokens and join them with whitespace.\n",
    "for i in range(answer_start + 1, answer_end + 1):\n",
    "    \n",
    "    # If it's a subword token, then recombine it with the previous token.\n",
    "    if question_raw[i][0:2] == '##':\n",
    "        question += question_raw[i][2:]\n",
    "    \n",
    "    # Otherwise, add a space then the token.\n",
    "    else:\n",
    "        question += ' ' + question_raw[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Question:  [CLS] which method is proseek based on ? [SEP] we used proximity extension immunoassay ( pea , proseek multiplex , olink ) to assess the serum levels of ninety - two inflammation - related proteins in czech patients with sle ( n = 75 ) and age - matched healthy control subjects ( n = 23 ) . [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\nAnswer:  proximity extension immunoassay\n"
    }
   ],
   "source": [
    "print(\"Question: \",question)\n",
    "print(\"Answer: \",answer_list_test[n_question])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "...Evaluate\nSample:  0 / 1\n[['proximity extension immunoassay', 'proximity extension immunoassay ( pea , proseek multiplex', 'proximity extension immunoassay ( pea , proseek multiplex , olink', 'proximity extension', 'proximity']]\n{'strict_accuracy': 1.0, 'lenient_accuracy': 1.0, 'mean_reciprocal_rank': 1.0}\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'strict_accuracy': 1.0, 'lenient_accuracy': 1.0, 'mean_reciprocal_rank': 1.0}"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "test_factoid_model(model,tokenizer,[[x_data_test[0][n_question]],[x_data_test[1][n_question]],[x_data_test[2][n_question]]],[answer_list_test[n_question]])"
   ]
  }
 ]
}