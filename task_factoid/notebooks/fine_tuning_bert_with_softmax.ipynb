{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import del modello creato e aggiunta del layer softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig, BertForQuestionAnswering, BertModel, BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    " tokenizer = BertTokenizer.from_pretrained(\"./transformers_models/biobert_factoid_pytorch\")\n",
    " encoder = TFBertModel.from_pretrained(\"./transformers_models/biobert_factoid_pytorch\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## QA Model\n",
    "max_len = 62\n",
    "input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "embedding = encoder(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
    ")[0]\n",
    "\n",
    "start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
    "start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs=[input_ids, token_type_ids, attention_mask],\n",
    "    outputs=[start_probs, end_probs],\n",
    ")\n",
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = keras.optimizers.Adam(lr=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=[loss, loss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione domande"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import di alcune domande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1, text1 = \"List signaling molecules (ligands) that interact with the receptor EGFR?\", \"the epidermal growth factor receptor (EGFR) ligands, such as epidermal growth factor (EGF) and amphiregulin (AREG)\"\n",
    "question2, text2 = \"List signaling molecules (ligands) that interact with the receptor EGFR?\", \"the epidermal growth factor receptor (EGFR) ligands, such as epidermal growth factor (EGF) and amphiregulin (AREG)\"\n",
    "dataset_raw=[[question1,text1],[question2,text2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['List signaling molecules (ligands) that interact with the receptor EGFR?',\n 'the epidermal growth factor receptor (EGFR) ligands, such as epidermal growth factor (EGF) and amphiregulin (AREG)']"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "dataset_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_list=[]\n",
    "token_type_ids_list=[]\n",
    "attention_mask_list=[]\n",
    "start_aswer_list=[]\n",
    "end_answer_list=[]\n",
    "\n",
    "for sample in dataset_raw:\n",
    "    encoding = tokenizer.encode_plus(sample[0], sample[1])\n",
    "    input_ids, token_type_ids = encoding[\"input_ids\"], encoding[\"token_type_ids\"]\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Aggiunta manuale start token end token\n",
    "    start_aswer_list.append(4)\n",
    "    end_answer_list.append(5)\n",
    "    \n",
    "    #Altre info\n",
    "    input_ids_list.append(input_ids)\n",
    "    token_type_ids_list.append(token_type_ids)\n",
    "    attention_mask_list.append(attention_mask)\n",
    "    \n",
    "input_ids_list=np.array(input_ids_list)\n",
    "token_type_ids_list=np.array(token_type_ids_list)\n",
    "attention_mask_list=np.array(attention_mask_list)\n",
    "start_aswer_list=np.array(start_aswer_list)\n",
    "end_answer_list=np.array(end_answer_list)\n",
    "\n",
    "x_data=[input_ids_list,token_type_ids_list,attention_mask_list]\n",
    "y_data=[start_aswer_list,end_answer_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 2 samples\nEpoch 1/10\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\nWARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n2/2 - 26s - loss: 8.6288 - activation_7_loss: 4.7456 - activation_8_loss: 3.8832\nEpoch 2/10\n2/2 - 2s - loss: 5.0573 - activation_7_loss: 2.7610 - activation_8_loss: 2.2964\nEpoch 3/10\n2/2 - 2s - loss: 2.0552 - activation_7_loss: 1.1600 - activation_8_loss: 0.8953\nEpoch 4/10\n2/2 - 2s - loss: 0.8314 - activation_7_loss: 0.3540 - activation_8_loss: 0.4774\nEpoch 5/10\n2/2 - 2s - loss: 0.2729 - activation_7_loss: 0.1179 - activation_8_loss: 0.1550\nEpoch 6/10\n2/2 - 2s - loss: 0.0999 - activation_7_loss: 0.0521 - activation_8_loss: 0.0478\nEpoch 7/10\n2/2 - 2s - loss: 0.0327 - activation_7_loss: 0.0173 - activation_8_loss: 0.0154\nEpoch 8/10\n2/2 - 2s - loss: 0.0117 - activation_7_loss: 0.0065 - activation_8_loss: 0.0052\nEpoch 9/10\n2/2 - 2s - loss: 0.0157 - activation_7_loss: 0.0046 - activation_8_loss: 0.0111\nEpoch 10/10\n2/2 - 2s - loss: 0.0075 - activation_7_loss: 0.0028 - activation_8_loss: 0.0048\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x17b3f10e608>"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "model.fit(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    epochs=10,  # For demonstration, 3 epochs are recommended\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit61209d9bcfea452998e2771b9ee1a7fb",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}