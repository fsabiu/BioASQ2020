{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('C:/Users/mgabr/Desktop/HLT/project/BioASQ2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-116-92d7e61cd2cd>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-116-92d7e61cd2cd>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    ${fileDirname}\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "${fileDirname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il modello per le liste si basa sullo stesso modello utilizzato per le factoid provvediamo a importare il modello corretto e facciamo un test sul funzionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "Model name './transformers_models/biobert_factoid_pytorch' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed './transformers_models/biobert_factoid_pytorch' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-88afabe5086e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./transformers_models/biobert_factoid_pytorch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertForQuestionAnswering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./transformers_models/biobert_factoid_pytorch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \"\"\"\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\transformers\\tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    494\u001b[0m                     \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms3_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                     \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_files_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m                 )\n\u001b[0;32m    498\u001b[0m             )\n",
      "\u001b[1;31mOSError\u001b[0m: Model name './transformers_models/biobert_factoid_pytorch' was not found in tokenizers model name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). We assumed './transformers_models/biobert_factoid_pytorch' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.txt'] but couldn't find such vocabulary files at this path or url."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./transformers_models/biobert_factoid_pytorch\")\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./transformers_models/biobert_factoid_pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La risposta corretta contiene queste entità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[['epidermal growth factor'],\n ['betacellulin'],\n ['epiregulin'],\n ['heparin-binding epidermal growth factor'],\n ['transforming growth factor-α'],\n ['amphiregulin'],\n ['epigen']]"
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "from utils.data import load_data_list, clean_synonyms\n",
    "lista_domande=load_data_list(\"./data/training8b.json\")\n",
    "lista_domande[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definisco la funzione per stampare la risposta corretta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_answer(answer_start,answer_end,all_tokens):# Start with the first token.\n",
    "    answer = all_tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if all_tokens[i][0:2] == '##':\n",
    "            answer += all_tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + all_tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', 'list', 'signaling', 'molecules', '(', 'l', '##igan', '##ds', ')', 'that', 'interact', 'with', 'the', 'receptor', 'e', '##g', '##f', '##r', '?', '[SEP]', 'the', 'e', '##pid', '##er', '##mal', 'growth', 'factor', 'receptor', '(', 'e', '##g', '##f', '##r', ')', 'l', '##igan', '##ds', ',', 'such', 'as', 'e', '##pid', '##er', '##mal', 'growth', 'factor', '(', 'e', '##g', '##f', ')', 'and', 'am', '##phi', '##re', '##gu', '##lin', '(', 'are', '##g', ')', '[SEP]']\n[101, 2190, 16085, 10799, 113, 181, 10888, 3680, 114, 1115, 12254, 1114, 1103, 10814, 174, 1403, 2087, 1197, 136, 102, 1103, 174, 25786, 1200, 7435, 3213, 5318, 10814, 113, 174, 1403, 2087, 1197, 114, 181, 10888, 3680, 117, 1216, 1112, 174, 25786, 1200, 7435, 3213, 5318, 113, 174, 1403, 2087, 114, 1105, 1821, 27008, 1874, 13830, 2836, 113, 1132, 1403, 114, 102]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
    }
   ],
   "source": [
    "question, text = \"List signaling molecules (ligands) that interact with the receptor EGFR?\", \"the epidermal growth factor receptor (EGFR) ligands, such as epidermal growth factor (EGF) and amphiregulin (AREG)\"\n",
    "encoding = tokenizer.encode_plus(question, text)\n",
    "input_ids, token_type_ids = encoding[\"input_ids\"], encoding[\"token_type_ids\"]\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(all_tokens)\n",
    "print(input_ids)\n",
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________\n",
    "## 1 Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dato il vettore della softmax per la risposta prendiamo tutte le parole che riportano uno score superiore a un determinato threshold. Prendiamo poi il minimo del vettore dello start e il massimo del vettore dell'end e poi utilizziamo un NER per identificare le entità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-3.5126, -7.0865, -6.9067, -7.7472, -7.6168, -6.4265, -7.8138, -8.5346,\n         -8.5282, -7.4421, -7.2022, -7.1606, -7.4848, -8.1219, -7.4963, -7.8732,\n         -8.4675, -8.8533, -7.7126, -7.8307, -2.8615, -3.0179, -6.7289, -6.7677,\n         -6.4157, -6.0162, -6.7201, -4.8849, -4.4312, -5.3506, -7.3976, -7.3558,\n         -6.2990, -4.9410, -2.0951, -5.2530, -4.9682, -5.6626, -3.1414, -6.0405,\n         -0.9388, -7.2085, -7.6842, -6.7369, -5.8507, -6.5148, -5.1844, -4.1726,\n         -7.7461, -7.0326, -7.0507, -7.4014, -1.8171, -7.4988, -7.5553, -7.1843,\n         -5.8848, -5.8190, -6.7066, -7.4752, -6.2458, -7.8307]],\n       grad_fn=<SqueezeBackward1>)\ntensor([[-2.4085, -8.4290, -8.2037, -6.7629, -8.2680, -8.3713, -8.0804, -6.4559,\n         -7.4831, -8.3483, -8.1218, -8.0534, -8.5519, -7.9593, -8.7100, -8.3001,\n         -7.9757, -6.8480, -7.3486, -2.5235, -5.5465, -5.8049, -7.4287, -7.5961,\n         -7.3014, -6.4549, -4.0458, -2.8896, -5.6771, -8.3694, -8.4386, -8.0441,\n         -4.8070, -4.1241, -5.4386, -6.1758,  0.0400, -3.3661, -4.7527, -5.2686,\n         -6.2663, -8.5304, -8.7012, -8.2546, -6.9668, -2.9589, -6.2427, -8.1993,\n         -8.7458, -4.7270, -3.6925, -7.0831, -5.8833, -7.6268, -7.9693, -6.8034,\n         -0.4863, -7.1200, -9.0028, -3.6532, -1.8109, -2.5235]],\n       grad_fn=<SqueezeBackward1>)\ntensor([[40, 52, 34]])\ntensor([[36, 56, 60]])\n"
    }
   ],
   "source": [
    "start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "print(start_scores)\n",
    "print(end_scores)\n",
    "values_start, indices_start=torch.topk(start_scores,3)\n",
    "values_end, indices_end=torch.topk(end_scores,3)\n",
    "print(indices_start)\n",
    "print(indices_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiamo prendendo l'indice più basso per lo start e quello maggiore per l'end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Answer: \"ligands , such as epidermal growth factor ( egf ) and amphiregulin\"\n"
    }
   ],
   "source": [
    "print_answer(34,56,all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A questo punto si dovrebbe fare NER per identificare le entità"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________\n",
    "## 2 Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta ottenuto il vettore con gli score si selezionanano le combinazioni di start e end che riportano uno score più alto. Si fa una semplice somma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-3.5126, -7.0865, -6.9067, -7.7472, -7.6168, -6.4265, -7.8138, -8.5346,\n         -8.5282, -7.4421, -7.2022, -7.1606, -7.4848, -8.1219, -7.4963, -7.8732,\n         -8.4675, -8.8533, -7.7126, -7.8307, -2.8615, -3.0179, -6.7289, -6.7677,\n         -6.4157, -6.0162, -6.7201, -4.8849, -4.4312, -5.3506, -7.3976, -7.3558,\n         -6.2990, -4.9410, -2.0951, -5.2530, -4.9682, -5.6626, -3.1414, -6.0405,\n         -0.9388, -7.2085, -7.6842, -6.7369, -5.8507, -6.5148, -5.1844, -4.1726,\n         -7.7461, -7.0326, -7.0507, -7.4014, -1.8171, -7.4988, -7.5553, -7.1843,\n         -5.8848, -5.8190, -6.7066, -7.4752, -6.2458, -7.8307]],\n       grad_fn=<SqueezeBackward1>)\ntensor([[-2.4085, -8.4290, -8.2037, -6.7629, -8.2680, -8.3713, -8.0804, -6.4559,\n         -7.4831, -8.3483, -8.1218, -8.0534, -8.5519, -7.9593, -8.7100, -8.3001,\n         -7.9757, -6.8480, -7.3486, -2.5235, -5.5465, -5.8049, -7.4287, -7.5961,\n         -7.3014, -6.4549, -4.0458, -2.8896, -5.6771, -8.3694, -8.4386, -8.0441,\n         -4.8070, -4.1241, -5.4386, -6.1758,  0.0400, -3.3661, -4.7527, -5.2686,\n         -6.2663, -8.5304, -8.7012, -8.2546, -6.9668, -2.9589, -6.2427, -8.1993,\n         -8.7458, -4.7270, -3.6925, -7.0831, -5.8833, -7.6268, -7.9693, -6.8034,\n         -0.4863, -7.1200, -9.0028, -3.6532, -1.8109, -2.5235]],\n       grad_fn=<SqueezeBackward1>)\n"
    }
   ],
   "source": [
    "start_scores, end_scores = model(torch.tensor([input_ids]), token_type_ids=torch.tensor([token_type_ids]))\n",
    "print(start_scores)\n",
    "print(end_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math  \n",
    "from queue import PriorityQueue \n",
    "def KMaxCombinations( arr1, arr2, dim, K): \n",
    "      \n",
    "    # max heap. \n",
    "    pq = PriorityQueue()  \n",
    "      \n",
    "    # insert all the possible   \n",
    "    # combinations in max heap. \n",
    "    for i in range(0, dim): \n",
    "        for j in range(0, dim): \n",
    "           a = arr1[i] + arr2[j]  \n",
    "           pq.put((-a, a,i,j)) \n",
    "              \n",
    "    # pop first N elements from  \n",
    "    # max heap and display them. \n",
    "    count = 0\n",
    "    while (count < K): \n",
    "        print(pq.get()[0:4]) \n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(0.89879876, -0.89879876, 40, 36)\n(1.4251255, -1.4251255, 40, 56)\n(1.7770776, -1.7770776, 52, 36)\n(2.0551105, -2.0551105, 34, 36)\n(2.3034043, -2.3034043, 52, 56)\n"
    }
   ],
   "source": [
    "KMaxCombinations(start_scores.detach().numpy()[0],end_scores.detach().numpy()[0],len(end_scores.detach().numpy()[0]),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stampiamo per esempio le prime 3 entità escludendo quelle con lo start maggiore dell'end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Answer: \"epidermal growth factor ( egf ) and amphiregulin\"\nAnswer: \"ligands\"\nAnswer: \"amphiregulin\"\n"
    }
   ],
   "source": [
    "print_answer(40,56,all_tokens)\n",
    "print_answer(34,36,all_tokens)\n",
    "print_answer(52,56,all_tokens)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit61209d9bcfea452998e2771b9ee1a7fb",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}